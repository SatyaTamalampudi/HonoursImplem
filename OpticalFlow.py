#!/usr/bin/env python
# coding: utf-8

import numpy as np
import os

#importing opencv for feature extraction
import cv2
from PIL import Image

from numpy import save
from numpy import load

import matplotlib.pyplot as plt

print(os.getcwd()) 
# Folder which contains all the images from which video is to be generated 
path = "C:\\Users\\1609653\\AnomalyDetection\DividedDataSet\\Training\\Test020"
os.chdir(path)

images = [img for img in os.listdir(path) 
if img.endswith(".tif") or
    img.endswith(".jpeg") or
    img.endswith("png")] 
print(images)

mag_videoarray_train = [] # a list which holds the magnitude values of all videos in training set
mag_videoarray_test = []  # a list which holds the magnitude values of all videos in testing set


# ## 2. Dense Optical Flow

# The video feed is read in as a VideoCapture object
cap = cv2.VideoCapture("mygeneratedvideo20.mp4")

# ret = a boolean return value from getting the frame, frame1 = the first frame in the entire video sequence
ret, frame1 = cap.read()
# Converts frame to grayscale because we only need the luminance channel for detecting edges - less computationally expensive
prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
# Creates an image filled with zero intensities with the same type and shape as the frame
hsv = np.zeros_like(frame1)
# Sets image saturation to maximum
hsv[...,1] = 255

ret, frame2 = cap.read()
mag_fullarray = []

while(ret):
    # Converts each frame to grayscale - we previously only converted the first frame to grayscale
    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
                                  
    # Calculates dense optical flow by Farneback method
    # https://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback
    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)

    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
    #print(mag1)
    flattened_image = mag.flatten() # flatten the whole array as one input instead of 238 inputs
    mag_fullarray.append(flattened_image) # Append the mag values of each image into a list
    
    # Sets image hue according to the optical flow direction
    hsv[...,0] = ang*180/np.pi/2
    # Sets image value according to the optical flow magnitude (normalized)
    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
    # Converts HSV to RGB (BGR) color representation
    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
    
    # Opens a new window and displays the output frame   
    cv2.imshow('frame2',rgb)
    # Frames are read by intervals of 30 millisecond. The programs breaks out of the while loop when the user presses the 'q' key
    k = cv2.waitKey(30) & 0xff
    if k == 27:
        break
    elif k == ord('s'):    
        prvs = next
    ret, frame2 = cap.read()

# The following frees up resources and closes all windows
cap.release()
cv2.destroyAllWindows()


# ## 3. Sparse Optical Flow

cap = cv2.VideoCapture('mygeneratedvideo20.mp4')
# params for ShiTomasi corner detection
feature_params = dict( maxCorners = 25,
                       qualityLevel = 0.2,
                       minDistance = 4,
                       blockSize = 7 )

# Parameters for lucas kanade optical flow
lk_params = dict( winSize  = (15,15),
                  maxLevel = 2,
                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

# Create some random colors
color = np.random.randint(0,255,(100,3))

# Take first frame and find corners in it
ret, old_frame = cap.read()
old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)

# Create a mask image for drawing purposes
mask = np.zeros_like(old_frame)
count=0
index = 0;
ret,frame = cap.read()
while(ret):
   
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # calculate optical flow
    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)
    # Select good points
    good_new = p1[st==1]
    good_old = p0[st==1]
    
    # draw the tracks
    for i,(new,old) in enumerate(zip(good_new,good_old)):        
        a,b = new.ravel()
        c,d = old.ravel()
        
        mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)
        frame = cv2.circle(frame,(a,b),5,color[i].tolist(),-1)
    img = cv2.add(frame,mask)

    cv2.imshow('frame',img)
    k = cv2.waitKey(30) & 0xff
    if k == 27:
        break

    # Now update the previous frame and previous points
    old_gray = frame_gray.copy()
    p0 = good_new.reshape(-1,1,2)
    #print(p0)
    ret,frame = cap.read()
    
cv2.destroyAllWindows()
cap.release()


##Generating histograms for the magnitude value generated by optical flow to detect anomaly
length = len(mag_fullarray)
for i in range (length):
    
    mag_array = mag_fullarray[i] 
    img = images[i+1]
    im = Image.open(img)
    imarray = np.array(im)
    image_array = np.asfarray(imarray).flatten().reshape((158,238))
    plt.subplot(2,2,1)
    plt.imshow(image_array)
    plt.title("Image")
    
    plt.subplot(2,2,2)
    plt.hist(mag_array,bins=32,cumulative=False)
    plt.title("Histogram")
   
    plt.show()
    hist=np.histogram(mag_array,bins=32)

## Collecting magnitude values generated by optial flow method into one array
## Appending all the arrays into one big array

mag_array = np.array(mag_fullarray)
print(mag_array.shape)
print(len(mag_array))
flatten_video = mag_array.flatten() 
flatten_video.reshape((199,37604))
mag_videoarray_train.append(flatten_video)



save('training_data.npy',mag_videoarray_train) ##Saving training data
save('testing_data.npy',mag_videoarray_test) ##Saving testing data

